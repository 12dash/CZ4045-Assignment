{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df_raw = pd.read_csv('../Data/review.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df_raw.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11  "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization using split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: x.split())\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  0.29001903533935547\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[We, had, my, Mother's, Birthday, Party, here,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[Good, Korean, grill, near, Eaton, Centre., Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[Was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[Ambience:, Would, not, expect, something, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[Absolutely, the, WORST, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [We, had, my, Mother's, Birthday, Party, here,...  \n",
       "1  [Good, Korean, grill, near, Eaton, Centre., Th...  \n",
       "2  [Was, recommended, to, try, this, place, by, f...  \n",
       "3  [Ambience:, Would, not, expect, something, thi...  \n",
       "4  [Absolutely, the, WORST, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df.to_csv('../Results/Tokenize_split.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: re.findall(\"[\\w']+\", x))\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  0.5477030277252197\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[We, had, my, Mother's, Birthday, Party, here,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[Good, Korean, grill, near, Eaton, Centre, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[Was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[Ambience, Would, not, expect, something, this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[Absolutely, the, WORST, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [We, had, my, Mother's, Birthday, Party, here,...  \n",
       "1  [Good, Korean, grill, near, Eaton, Centre, The...  \n",
       "2  [Was, recommended, to, try, this, place, by, f...  \n",
       "3  [Ambience, Would, not, expect, something, this...  \n",
       "4  [Absolutely, the, WORST, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df.to_csv('../Results/Tokenize_regex.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jay/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  17.022397994995117\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[We, had, my, Mother, 's, Birthday, Party, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[Good, Korean, grill, near, Eaton, Centre, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[Was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[Ambience, :, Would, not, expect, something, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[Absolutely, the, WORST, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [We, had, my, Mother, 's, Birthday, Party, her...  \n",
       "1  [Good, Korean, grill, near, Eaton, Centre, ., ...  \n",
       "2  [Was, recommended, to, try, this, place, by, f...  \n",
       "3  [Ambience, :, Would, not, expect, something, t...  \n",
       "4  [Absolutely, the, WORST, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df.to_csv('../Results/Tokenize_nltk.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# POS Tagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "sample_df = df.sample(n = 5, random_state=1)\n",
    "sample_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>6Q4QDM0K_AIlX6w7jPo61g</td>\n",
       "      <td>SsbfgJ-lUhHc3lN1NrPavQ</td>\n",
       "      <td>AT5NZHqoLeP4rhWMM2OWBA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Love this place!  Pizza is awesome!    Wish th...</td>\n",
       "      <td>2018-06-22 01:16:36</td>\n",
       "      <td>[Love, this, place, !, Pizza, is, awesome, !, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>_kY2RF2iLprkVpLAQK1d2w</td>\n",
       "      <td>Opkt4zA7HcRxTdQPC0Ey7g</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing restaurant that I ever been. The meat ...</td>\n",
       "      <td>2017-01-13 05:51:43</td>\n",
       "      <td>[Amazing, restaurant, that, I, ever, been, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>KG9xyGuOFHGcpNNAPOqF7A</td>\n",
       "      <td>tOdJethFY2mgwwwmDjYrkQ</td>\n",
       "      <td>cOY6ipigtTXdcmmmiFiniA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Perk-Cup is definitely a convenient spot to dr...</td>\n",
       "      <td>2016-03-24 11:01:57</td>\n",
       "      <td>[Perk-Cup, is, definitely, a, convenient, spot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>QsMM-vF3vLDb6Z1jTfokiA</td>\n",
       "      <td>xEpaUH5WYB-sEuWtAcTBRw</td>\n",
       "      <td>suJ-eo5Gkvcg0iXVQw4RyA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grade A+ times 10. Staff was one of the best i...</td>\n",
       "      <td>2012-03-07 03:55:49</td>\n",
       "      <td>[Grade, A+, times, 10, ., Staff, was, one, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>5nrFh8zwCBluHAbpTmiXaw</td>\n",
       "      <td>Q0CkxRm-1UixyY6Gc9ZWqA</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>So very disappointed in Texas Roadhouse today....</td>\n",
       "      <td>2018-05-13 20:40:51</td>\n",
       "      <td>[So, very, disappointed, in, Texas, Roadhouse,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "7537  6Q4QDM0K_AIlX6w7jPo61g  SsbfgJ-lUhHc3lN1NrPavQ  AT5NZHqoLeP4rhWMM2OWBA   \n",
       "4558  _kY2RF2iLprkVpLAQK1d2w  Opkt4zA7HcRxTdQPC0Ey7g  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "6636  KG9xyGuOFHGcpNNAPOqF7A  tOdJethFY2mgwwwmDjYrkQ  cOY6ipigtTXdcmmmiFiniA   \n",
       "9707  QsMM-vF3vLDb6Z1jTfokiA  xEpaUH5WYB-sEuWtAcTBRw  suJ-eo5Gkvcg0iXVQw4RyA   \n",
       "4015  5nrFh8zwCBluHAbpTmiXaw  Q0CkxRm-1UixyY6Gc9ZWqA  Rii85bzYKGC9P0zOyAem6A   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "7537    3.0       0      0     0   \n",
       "4558    5.0       0      0     0   \n",
       "6636    3.0       1      0     0   \n",
       "9707    5.0       1      0     0   \n",
       "4015    1.0       0      0     0   \n",
       "\n",
       "                                                   text                 date  \\\n",
       "7537  Love this place!  Pizza is awesome!    Wish th...  2018-06-22 01:16:36   \n",
       "4558  Amazing restaurant that I ever been. The meat ...  2017-01-13 05:51:43   \n",
       "6636  Perk-Cup is definitely a convenient spot to dr...  2016-03-24 11:01:57   \n",
       "9707  Grade A+ times 10. Staff was one of the best i...  2012-03-07 03:55:49   \n",
       "4015  So very disappointed in Texas Roadhouse today....  2018-05-13 20:40:51   \n",
       "\n",
       "                                        Tokenized words  \n",
       "7537  [Love, this, place, !, Pizza, is, awesome, !, ...  \n",
       "4558  [Amazing, restaurant, that, I, ever, been, ., ...  \n",
       "6636  [Perk-Cup, is, definitely, a, convenient, spot...  \n",
       "9707  [Grade, A+, times, 10, ., Staff, was, one, of,...  \n",
       "4015  [So, very, disappointed, in, Texas, Roadhouse,...  "
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "texts = sample_df[\"text\"]\n",
    "tokens = sample_df[\"Tokenized words\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# NLTK\n",
    "for i in range(5):\n",
    "    text = texts.iloc[i]\n",
    "    pos_tag = nltk.pos_tag(tokens.iloc[i])\n",
    "    \n",
    "    print(text)\n",
    "    print(pos_tag)\n",
    "    print(\"----\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Love this place!  Pizza is awesome!    Wish they could provide the Ass Clown \"Orange Citrus\" beer on a regular basis that is on their menu.  Great local beer!   The last times we visited it was not available.\n",
      "[('Love', 'VB'), ('this', 'DT'), ('place', 'NN'), ('!', '.'), ('Pizza', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('!', '.'), ('Wish', 'JJ'), ('they', 'PRP'), ('could', 'MD'), ('provide', 'VB'), ('the', 'DT'), ('Ass', 'NNP'), ('Clown', 'NNP'), ('``', '``'), ('Orange', 'NNP'), ('Citrus', 'NNP'), (\"''\", \"''\"), ('beer', 'NN'), ('on', 'IN'), ('a', 'DT'), ('regular', 'JJ'), ('basis', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('on', 'IN'), ('their', 'PRP$'), ('menu', 'NN'), ('.', '.'), ('Great', 'NNP'), ('local', 'JJ'), ('beer', 'NN'), ('!', '.'), ('The', 'DT'), ('last', 'JJ'), ('times', 'NNS'), ('we', 'PRP'), ('visited', 'VBD'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('available', 'JJ'), ('.', '.')]\n",
      "----\n",
      "Amazing restaurant that I ever been. The meat and the vegetables are really fresh. The service was really nice and patient since this is the first time I went there and he introduced so many good stuffs. The watermelon juice surprise me, it's soooooo good. The meat quality is good and the service is quick, they have a wide selection of the food. I was also very  impressed with the service, he checking on us often. I would like to go to this place again and again and I really love it!!\n",
      "[('Amazing', 'VBG'), ('restaurant', 'NN'), ('that', 'IN'), ('I', 'PRP'), ('ever', 'RB'), ('been', 'VBN'), ('.', '.'), ('The', 'DT'), ('meat', 'NN'), ('and', 'CC'), ('the', 'DT'), ('vegetables', 'NNS'), ('are', 'VBP'), ('really', 'RB'), ('fresh', 'JJ'), ('.', '.'), ('The', 'DT'), ('service', 'NN'), ('was', 'VBD'), ('really', 'RB'), ('nice', 'JJ'), ('and', 'CC'), ('patient', 'JJ'), ('since', 'IN'), ('this', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('time', 'NN'), ('I', 'PRP'), ('went', 'VBD'), ('there', 'RB'), ('and', 'CC'), ('he', 'PRP'), ('introduced', 'VBD'), ('so', 'RB'), ('many', 'JJ'), ('good', 'JJ'), ('stuffs', 'NNS'), ('.', '.'), ('The', 'DT'), ('watermelon', 'NN'), ('juice', 'NN'), ('surprise', 'NN'), ('me', 'PRP'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('soooooo', 'JJ'), ('good', 'JJ'), ('.', '.'), ('The', 'DT'), ('meat', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('good', 'JJ'), ('and', 'CC'), ('the', 'DT'), ('service', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), (',', ','), ('they', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('wide', 'JJ'), ('selection', 'NN'), ('of', 'IN'), ('the', 'DT'), ('food', 'NN'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('also', 'RB'), ('very', 'RB'), ('impressed', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('service', 'NN'), (',', ','), ('he', 'PRP'), ('checking', 'VBG'), ('on', 'IN'), ('us', 'PRP'), ('often', 'RB'), ('.', '.'), ('I', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('go', 'VB'), ('to', 'TO'), ('this', 'DT'), ('place', 'NN'), ('again', 'RB'), ('and', 'CC'), ('again', 'RB'), ('and', 'CC'), ('I', 'PRP'), ('really', 'RB'), ('love', 'VB'), ('it', 'PRP'), ('!', '.'), ('!', '.')]\n",
      "----\n",
      "Perk-Cup is definitely a convenient spot to drop by if you want something different or healthy without getting out of your car. However, I didn't have the best experience. I ordered my food in advance and arrived when they told me my food would be ready. When I first arrived at the drive up, I was food was still being prepared- that's fine; I didn't mind waiting a few minutes. I ended up waiting 10+ minutes at the window. Because or the issue, I was given a free cookie. \n",
      "\n",
      "All of this to say, the food isn't bad but I wouldn't bet on your food being ready when you get there!\n",
      "[('Perk-Cup', 'NNP'), ('is', 'VBZ'), ('definitely', 'RB'), ('a', 'DT'), ('convenient', 'NN'), ('spot', 'NN'), ('to', 'TO'), ('drop', 'VB'), ('by', 'IN'), ('if', 'IN'), ('you', 'PRP'), ('want', 'VBP'), ('something', 'NN'), ('different', 'JJ'), ('or', 'CC'), ('healthy', 'JJ'), ('without', 'IN'), ('getting', 'VBG'), ('out', 'IN'), ('of', 'IN'), ('your', 'PRP$'), ('car', 'NN'), ('.', '.'), ('However', 'RB'), (',', ','), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('have', 'VB'), ('the', 'DT'), ('best', 'JJS'), ('experience', 'NN'), ('.', '.'), ('I', 'PRP'), ('ordered', 'VBD'), ('my', 'PRP$'), ('food', 'NN'), ('in', 'IN'), ('advance', 'NN'), ('and', 'CC'), ('arrived', 'VBD'), ('when', 'WRB'), ('they', 'PRP'), ('told', 'VBD'), ('me', 'PRP'), ('my', 'PRP$'), ('food', 'NN'), ('would', 'MD'), ('be', 'VB'), ('ready', 'JJ'), ('.', '.'), ('When', 'WRB'), ('I', 'PRP'), ('first', 'RB'), ('arrived', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('drive', 'NN'), ('up', 'RP'), (',', ','), ('I', 'PRP'), ('was', 'VBD'), ('food', 'NN'), ('was', 'VBD'), ('still', 'RB'), ('being', 'VBG'), ('prepared-', 'JJ'), ('that', 'DT'), (\"'s\", 'VBZ'), ('fine', 'JJ'), (';', ':'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('mind', 'VB'), ('waiting', 'VBG'), ('a', 'DT'), ('few', 'JJ'), ('minutes', 'NNS'), ('.', '.'), ('I', 'PRP'), ('ended', 'VBD'), ('up', 'RB'), ('waiting', 'VBG'), ('10+', 'CD'), ('minutes', 'NNS'), ('at', 'IN'), ('the', 'DT'), ('window', 'NN'), ('.', '.'), ('Because', 'IN'), ('or', 'CC'), ('the', 'DT'), ('issue', 'NN'), (',', ','), ('I', 'PRP'), ('was', 'VBD'), ('given', 'VBN'), ('a', 'DT'), ('free', 'JJ'), ('cookie', 'NN'), ('.', '.'), ('All', 'DT'), ('of', 'IN'), ('this', 'DT'), ('to', 'TO'), ('say', 'VB'), (',', ','), ('the', 'DT'), ('food', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), ('bet', 'VB'), ('on', 'IN'), ('your', 'PRP$'), ('food', 'NN'), ('being', 'VBG'), ('ready', 'JJ'), ('when', 'WRB'), ('you', 'PRP'), ('get', 'VBP'), ('there', 'RB'), ('!', '.')]\n",
      "----\n",
      "Grade A+ times 10. Staff was one of the best if my years traveling and staying at hotels. One of the cleanest and spacious rooms ever and if you leave some things home just go to the front desk and its free of charge.\n",
      "[('Grade', 'NNP'), ('A+', 'NNP'), ('times', 'VBZ'), ('10', 'CD'), ('.', '.'), ('Staff', 'NNP'), ('was', 'VBD'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('if', 'IN'), ('my', 'PRP$'), ('years', 'NNS'), ('traveling', 'VBG'), ('and', 'CC'), ('staying', 'VBG'), ('at', 'IN'), ('hotels', 'NNS'), ('.', '.'), ('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('cleanest', 'NN'), ('and', 'CC'), ('spacious', 'JJ'), ('rooms', 'NNS'), ('ever', 'RB'), ('and', 'CC'), ('if', 'IN'), ('you', 'PRP'), ('leave', 'VBP'), ('some', 'DT'), ('things', 'NNS'), ('home', 'NN'), ('just', 'RB'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('front', 'NN'), ('desk', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('free', 'JJ'), ('of', 'IN'), ('charge', 'NN'), ('.', '.')]\n",
      "----\n",
      "So very disappointed in Texas Roadhouse today. We may not ever be going back.\n",
      "\n",
      "It's Mother's Day, so I called this morning to see if they would take a reservation. The gal on the phone said they do call ahead seating 2 hours in advance, so if we wanted to come at 5:30, call at 3:30.\n",
      "\n",
      "3:30 rolls around and now we're being told they aren't taking seating for parties over 8 people. If we had been told that earlier, we could have made reservations elsewhere, but now we've been waiting around all day for this! Spoke to a manager who basically said 'I don't know what she told you that'\n",
      "\n",
      "You shouldn't have people answering the phones on Mother's Day if they're not aware of the policies. They also said they couldn't accommodate us in any way. 'I can tryyyy but you'd probably be at two tables'\n",
      "\n",
      "Now we're scrambling to find somewhere for Mother's Day and could not be more disappointed with the bad information from Texas Roadhouse.\n",
      "[('So', 'RB'), ('very', 'RB'), ('disappointed', 'JJ'), ('in', 'IN'), ('Texas', 'NNP'), ('Roadhouse', 'NNP'), ('today', 'NN'), ('.', '.'), ('We', 'PRP'), ('may', 'MD'), ('not', 'RB'), ('ever', 'RB'), ('be', 'VB'), ('going', 'VBG'), ('back', 'RB'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('Mother', 'NNP'), (\"'s\", 'POS'), ('Day', 'NNP'), (',', ','), ('so', 'IN'), ('I', 'PRP'), ('called', 'VBD'), ('this', 'DT'), ('morning', 'NN'), ('to', 'TO'), ('see', 'VB'), ('if', 'IN'), ('they', 'PRP'), ('would', 'MD'), ('take', 'VB'), ('a', 'DT'), ('reservation', 'NN'), ('.', '.'), ('The', 'DT'), ('gal', 'NN'), ('on', 'IN'), ('the', 'DT'), ('phone', 'NN'), ('said', 'VBD'), ('they', 'PRP'), ('do', 'VBP'), ('call', 'VB'), ('ahead', 'RB'), ('seating', 'VBG'), ('2', 'CD'), ('hours', 'NNS'), ('in', 'IN'), ('advance', 'NN'), (',', ','), ('so', 'RB'), ('if', 'IN'), ('we', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('come', 'VB'), ('at', 'IN'), ('5:30', 'CD'), (',', ','), ('call', 'NN'), ('at', 'IN'), ('3:30', 'CD'), ('.', '.'), ('3:30', 'CD'), ('rolls', 'NNS'), ('around', 'IN'), ('and', 'CC'), ('now', 'RB'), ('we', 'PRP'), (\"'re\", 'VBP'), ('being', 'VBG'), ('told', 'VBN'), ('they', 'PRP'), ('are', 'VBP'), (\"n't\", 'RB'), ('taking', 'VBG'), ('seating', 'VBG'), ('for', 'IN'), ('parties', 'NNS'), ('over', 'IN'), ('8', 'CD'), ('people', 'NNS'), ('.', '.'), ('If', 'IN'), ('we', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('told', 'VBN'), ('that', 'IN'), ('earlier', 'JJR'), (',', ','), ('we', 'PRP'), ('could', 'MD'), ('have', 'VB'), ('made', 'VBN'), ('reservations', 'NNS'), ('elsewhere', 'RB'), (',', ','), ('but', 'CC'), ('now', 'RB'), ('we', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('waiting', 'VBG'), ('around', 'IN'), ('all', 'DT'), ('day', 'NN'), ('for', 'IN'), ('this', 'DT'), ('!', '.'), ('Spoke', 'NNP'), ('to', 'TO'), ('a', 'DT'), ('manager', 'NN'), ('who', 'WP'), ('basically', 'RB'), ('said', 'VBD'), (\"'\", \"''\"), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('what', 'WP'), ('she', 'PRP'), ('told', 'VBD'), ('you', 'PRP'), (\"that'\", 'VBP'), ('You', 'PRP'), ('should', 'MD'), (\"n't\", 'RB'), ('have', 'VB'), ('people', 'NNS'), ('answering', 'VBG'), ('the', 'DT'), ('phones', 'NNS'), ('on', 'IN'), ('Mother', 'NNP'), (\"'s\", 'POS'), ('Day', 'NNP'), ('if', 'IN'), ('they', 'PRP'), (\"'re\", 'VBP'), ('not', 'RB'), ('aware', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('policies', 'NNS'), ('.', '.'), ('They', 'PRP'), ('also', 'RB'), ('said', 'VBD'), ('they', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('accommodate', 'VB'), ('us', 'PRP'), ('in', 'IN'), ('any', 'DT'), ('way', 'NN'), ('.', '.'), (\"'\", \"''\"), ('I', 'PRP'), ('can', 'MD'), ('tryyyy', 'VB'), ('but', 'CC'), ('you', 'PRP'), (\"'d\", 'MD'), ('probably', 'RB'), ('be', 'VB'), ('at', 'IN'), ('two', 'CD'), (\"tables'\", 'NNS'), ('Now', 'RB'), ('we', 'PRP'), (\"'re\", 'VBP'), ('scrambling', 'VBG'), ('to', 'TO'), ('find', 'VB'), ('somewhere', 'RB'), ('for', 'IN'), ('Mother', 'NNP'), (\"'s\", 'POS'), ('Day', 'NNP'), ('and', 'CC'), ('could', 'MD'), ('not', 'RB'), ('be', 'VB'), ('more', 'RBR'), ('disappointed', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('bad', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('Texas', 'NNP'), ('Roadhouse', 'NNP'), ('.', '.')]\n",
      "----\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SpaCy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i in range(5):\n",
    "    doc = nlp(texts.iloc[i])\n",
    "\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, end = \" \")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"----\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Love VERB this DET place NOUN ! PUNCT   SPACE Pizza PROPN is AUX awesome ADJ ! PUNCT     SPACE Wish VERB they PRON could AUX provide VERB the DET Ass PROPN Clown PROPN \" PUNCT Orange PROPN Citrus PROPN \" PUNCT beer NOUN on ADP a DET regular ADJ basis NOUN that DET is VERB on ADP their PRON menu NOUN . PUNCT   SPACE Great ADJ local ADJ beer NOUN ! PUNCT    SPACE The DET last ADJ times NOUN we PRON visited VERB it PRON was AUX not PART available ADJ . PUNCT \n",
      "----\n",
      "Amazing ADJ restaurant NOUN that DET I PRON ever ADV been VERB . PUNCT The DET meat NOUN and CCONJ the DET vegetables NOUN are AUX really ADV fresh ADJ . PUNCT The DET service NOUN was AUX really ADV nice ADJ and CCONJ patient ADJ since SCONJ this DET is VERB the DET first ADJ time NOUN I PRON went VERB there ADV and CCONJ he PRON introduced VERB so ADV many ADJ good ADJ stuffs NOUN . PUNCT The DET watermelon NOUN juice NOUN surprise VERB me PRON , PUNCT it PRON 's AUX soooooo ADJ good ADJ . PUNCT The DET meat NOUN quality NOUN is AUX good ADJ and CCONJ the DET service NOUN is VERB quick ADJ , PUNCT they PRON have VERB a DET wide ADJ selection NOUN of ADP the DET food NOUN . PUNCT I PRON was AUX also ADV very ADV   SPACE impressed ADJ with ADP the DET service NOUN , PUNCT he PRON checking VERB on ADP us PRON often ADV . PUNCT I PRON would AUX like VERB to PART go VERB to ADP this DET place NOUN again ADV and CCONJ again ADV and CCONJ I PRON really ADV love VERB it PRON ! PUNCT ! PUNCT \n",
      "----\n",
      "Perk PROPN - PUNCT Cup PROPN is AUX definitely ADV a DET convenient ADJ spot NOUN to PART drop VERB by ADP if SCONJ you PRON want VERB something PRON different ADJ or CCONJ healthy ADJ without ADP getting VERB out SCONJ of ADP your PRON car NOUN . PUNCT However ADV , PUNCT I PRON did AUX n't PART have VERB the DET best ADJ experience NOUN . PUNCT I PRON ordered VERB my PRON food NOUN in ADP advance NOUN and CCONJ arrived VERB when ADV they PRON told VERB me PRON my PRON food NOUN would AUX be VERB ready ADJ . PUNCT When ADV I PRON first ADV arrived VERB at ADP the DET drive NOUN up ADP , PUNCT I PRON was AUX food NOUN was AUX still ADV being VERB prepared- ADJ that DET 's AUX fine ADJ ; PUNCT I PRON did AUX n't PART mind VERB waiting VERB a DET few ADJ minutes NOUN . PUNCT I PRON ended VERB up ADP waiting VERB 10 NUM + NUM minutes NOUN at ADP the DET window NOUN . PUNCT Because SCONJ or CCONJ the DET issue NOUN , PUNCT I PRON was AUX given VERB a DET free ADJ cookie NOUN . PUNCT \n",
      "\n",
      " SPACE All DET of ADP this DET to PART say VERB , PUNCT the DET food NOUN is AUX n't PART bad ADJ but CCONJ I PRON would AUX n't PART bet VERB on ADP your PRON food NOUN being VERB ready ADJ when ADV you PRON get VERB there ADV ! PUNCT \n",
      "----\n",
      "Grade PROPN A+ PROPN times PROPN 10 NUM . PUNCT Staff NOUN was AUX one NUM of ADP the DET best ADJ if SCONJ my PRON years NOUN traveling VERB and CCONJ staying VERB at ADP hotels NOUN . PUNCT One NUM of ADP the DET cleanest ADJ and CCONJ spacious ADJ rooms NOUN ever ADV and CCONJ if SCONJ you PRON leave VERB some DET things NOUN home ADV just ADV go VERB to ADP the DET front ADJ desk NOUN and CCONJ its PRON free ADJ of ADP charge NOUN . PUNCT \n",
      "----\n",
      "So CCONJ very ADV disappointed ADJ in ADP Texas PROPN Roadhouse PROPN today NOUN . PUNCT We PRON may AUX not PART ever ADV be AUX going VERB back ADV . PUNCT \n",
      "\n",
      " SPACE It PRON 's AUX Mother PROPN 's PART Day PROPN , PUNCT so ADV I PRON called VERB this DET morning NOUN to PART see VERB if SCONJ they PRON would AUX take VERB a DET reservation NOUN . PUNCT The DET gal NOUN on ADP the DET phone NOUN said VERB they PRON do AUX call VERB ahead ADV seating VERB 2 NUM hours NOUN in ADP advance NOUN , PUNCT so CCONJ if SCONJ we PRON wanted VERB to PART come VERB at ADP 5:30 NUM , PUNCT call VERB at ADP 3:30 NUM . PUNCT \n",
      "\n",
      " SPACE 3:30 NUM rolls VERB around ADV and CCONJ now ADV we PRON 're AUX being AUX told VERB they PRON are AUX n't PART taking VERB seating NOUN for ADP parties NOUN over ADP 8 NUM people NOUN . PUNCT If SCONJ we PRON had AUX been AUX told VERB that SCONJ earlier ADV , PUNCT we PRON could AUX have AUX made VERB reservations NOUN elsewhere ADV , PUNCT but CCONJ now ADV we PRON 've AUX been AUX waiting VERB around ADV all DET day NOUN for ADP this DET ! PUNCT Spoke VERB to ADP a DET manager NOUN who PRON basically ADV said VERB ' PUNCT I PRON do AUX n't PART know VERB what PRON she PRON told VERB you PRON that SCONJ ' PUNCT \n",
      "\n",
      " SPACE You PRON should AUX n't PART have VERB people NOUN answering VERB the DET phones NOUN on ADP Mother PROPN 's PART Day NOUN if SCONJ they PRON 're VERB not PART aware ADJ of ADP the DET policies NOUN . PUNCT They PRON also ADV said VERB they PRON could AUX n't PART accommodate VERB us PRON in ADP any DET way NOUN . PUNCT ' PUNCT I PRON can AUX tryyyy VERB but CCONJ you PRON 'd AUX probably ADV be VERB at ADP two NUM tables NOUN ' PUNCT \n",
      "\n",
      " SPACE Now ADV we PRON 're AUX scrambling VERB to PART find VERB somewhere ADV for ADP Mother PROPN 's PART Day PROPN and CCONJ could AUX not PART be VERB more ADV disappointed ADJ with ADP the DET bad ADJ information NOUN from ADP Texas PROPN Roadhouse PROPN . PUNCT \n",
      "----\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Noun-Adjective Pairs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "random.seed(448)\n",
    "RANDOM_STATE = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "one_star_df = df.loc[df['stars'] == 1.0]\n",
    "dfs = dict(tuple(one_star_df.groupby('business_id')))\n",
    "business_ids = df['business_id'].unique().tolist()\n",
    "selected_ids = random.sample(business_ids, 50)\n",
    "\n",
    "one_star_reviews = []\n",
    "for id in selected_ids:\n",
    "    one_star_review = dfs[id].sample(n = 1, random_state = RANDOM_STATE)\n",
    "    one_star_reviews.append(one_star_review.iloc[0][\"text\"])\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "c = Counter(out)\n",
    "c.most_common(10)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('food', 'good'), 6),\n",
       " (('place', 'several'), 5),\n",
       " (('alternator', 'bad'), 4),\n",
       " (('service', 'poor'), 3),\n",
       " (('year', 'old'), 3),\n",
       " (('drink', 'sure'), 3),\n",
       " (('sandwich', 'more'), 3),\n",
       " (('bar', 'cheap'), 3),\n",
       " (('BBQ', 'sweet'), 3),\n",
       " (('sauce', 'sweet'), 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "n_star_df = df.loc[df['stars'] == 2.0]\n",
    "dfs = dict(tuple(n_star_df.groupby('business_id')))\n",
    "business_ids = df['business_id'].unique().tolist()\n",
    "selected_ids = random.sample(business_ids, 20)\n",
    "\n",
    "one_star_reviews = []\n",
    "for id in selected_ids:\n",
    "    one_star_review = dfs[id].sample(n = 1, random_state = RANDOM_STATE)\n",
    "    one_star_reviews.append(one_star_review.iloc[0][\"text\"])\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "c = Counter(out)\n",
    "c.most_common(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('minutes', 'busy'), 4),\n",
       " (('minutes', 'front'), 3),\n",
       " (('bar', 'few'), 2),\n",
       " (('attention', 'few'), 2),\n",
       " (('time', 'other'), 2),\n",
       " (('food', 'good'), 2),\n",
       " (('salad', 'good'), 2),\n",
       " (('shrimp', 'cooked'), 2),\n",
       " (('cocktail', 'cooked'), 2),\n",
       " (('salad', 'ok'), 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "n_star_df = df.loc[df['stars'] == 3.0]\n",
    "dfs = dict(tuple(n_star_df.groupby('business_id')))\n",
    "business_ids = df['business_id'].unique().tolist()\n",
    "selected_ids = random.sample(business_ids, 20)\n",
    "\n",
    "one_star_reviews = []\n",
    "for id in selected_ids:\n",
    "    one_star_review = dfs[id].sample(n = 1, random_state = RANDOM_STATE)\n",
    "    one_star_reviews.append(one_star_review.iloc[0][\"text\"])\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "c = Counter(out)\n",
    "c.most_common(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('car', 'OCD'), 3),\n",
       " (('service', 'professional'), 2),\n",
       " (('guy', 'professional'), 2),\n",
       " (('guys', 'OCD'), 2),\n",
       " (('service', 'slow'), 2),\n",
       " (('sauce', 'falafel'), 2),\n",
       " (('lunch', 'same'), 2),\n",
       " (('food', 'reasonable'), 2),\n",
       " (('wings', 'reasonable'), 2),\n",
       " (('time', 'reasonable'), 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "n_star_df = df.loc[df['stars'] == 4.0]\n",
    "dfs = dict(tuple(n_star_df.groupby('business_id')))\n",
    "business_ids = df['business_id'].unique().tolist()\n",
    "selected_ids = random.sample(business_ids, 20)\n",
    "\n",
    "one_star_reviews = []\n",
    "for id in selected_ids:\n",
    "    one_star_review = dfs[id].sample(n = 1, random_state = RANDOM_STATE)\n",
    "    one_star_reviews.append(one_star_review.iloc[0][\"text\"])\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "c = Counter(out)\n",
    "c.most_common(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('staff', 'friendly'), 3),\n",
       " (('try', 'lovely'), 2),\n",
       " (('Wings', 'only'), 2),\n",
       " (('Lamb', 'best'), 2),\n",
       " (('Burger', 'best'), 2),\n",
       " (('Salmon', 'more'), 2),\n",
       " (('Boston', 'more'), 2),\n",
       " (('burger', 'thin'), 2),\n",
       " (('bit', 'good'), 2),\n",
       " (('pizza', 'few'), 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "n_star_df = df.loc[df['stars'] == 5.0]\n",
    "dfs = dict(tuple(n_star_df.groupby('business_id')))\n",
    "business_ids = df['business_id'].unique().tolist()\n",
    "selected_ids = random.sample(business_ids, 20)\n",
    "\n",
    "one_star_reviews = []\n",
    "for id in selected_ids:\n",
    "    one_star_review = dfs[id].sample(n = 1, random_state = RANDOM_STATE)\n",
    "    one_star_reviews.append(one_star_review.iloc[0][\"text\"])\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "c = Counter(out)\n",
    "c.most_common(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('Pancakes', 'nice'), 4),\n",
       " (('staff', 'friendly'), 3),\n",
       " (('COLD', 'Thick'), 3),\n",
       " (('Joe', 'upbeat'), 2),\n",
       " (('service', 'prepared'), 2),\n",
       " (('team', 'prepared'), 2),\n",
       " (('pool', 'prepared'), 2),\n",
       " (('bacon', 'nice'), 2),\n",
       " (('place', 'good'), 2),\n",
       " (('bill', 'new'), 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "from itertools import chain\n",
    "one_star_reviews = [\"food amazing food amazing\"]\n",
    "\n",
    "noun_adj_pairs = []\n",
    "\n",
    "for i in range(len(one_star_reviews)):\n",
    "    doc = nlp(one_star_reviews[i])\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ':\n",
    "                noun_adj_pairs.append((token,doc[j]))\n",
    "                break\n",
    "\n",
    "out = []\n",
    "\n",
    "for pairs in noun_adj_pairs:\n",
    "    item_1 = pairs[0]\n",
    "    item_2 = pairs[1]\n",
    "    out.append((str(item_1), str(item_2)))\n",
    "\n",
    "print(out)\n",
    "\n",
    "c = Counter(out)\n",
    "c"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('food', 'amazing'), ('food', 'amazing')]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({('food', 'amazing'): 2})"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SpaCy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from spacy.lang.en import English"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "nlp = English()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: [i.text for i in nlp(x)])\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  9.867114067077637\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[We, had, my, Mother, 's, Birthday, Party, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[Good, Korean, grill, near, Eaton, Centre, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[Was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[Ambience, :, Would, not, expect, something, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[Absolutely, the, WORST, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [We, had, my, Mother, 's, Birthday, Party, her...  \n",
       "1  [Good, Korean, grill, near, Eaton, Centre, ., ...  \n",
       "2  [Was, recommended, to, try, this, place, by, f...  \n",
       "3  [Ambience, :, Would, not, expect, something, t...  \n",
       "4  [Absolutely, the, WORST, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df.to_csv('../Results/Tokenize_spacy.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: text_to_word_sequence(x))\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  0.6085107326507568\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[we, had, my, mother's, birthday, party, here,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[good, korean, grill, near, eaton, centre, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[ambience, would, not, expect, something, this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[absolutely, the, worst, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [we, had, my, mother's, birthday, party, here,...  \n",
       "1  [good, korean, grill, near, eaton, centre, the...  \n",
       "2  [was, recommended, to, try, this, place, by, f...  \n",
       "3  [ambience, would, not, expect, something, this...  \n",
       "4  [absolutely, the, worst, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df.to_csv('../Results/Tokenize_keras.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from gensim.utils import tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df = df_raw.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "t0 = time.time()\n",
    "df['Tokenized words'] = df['text'].apply(lambda x: [i for i in tokenize(x)])\n",
    "t1 = time.time()\n",
    "print(\"Time Taken : \", t1-t0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time Taken :  2.2377827167510986\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Tokenized words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "      <td>[We, had, my, Mother, s, Birthday, Party, here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "      <td>[Good, Korean, grill, near, Eaton, Centre, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "      <td>[Was, recommended, to, try, this, place, by, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "      <td>[Ambience, Would, not, expect, something, this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "      <td>[Absolutely, the, WORST, pool, company, that, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25   \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43   \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15   \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26   \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11   \n",
       "\n",
       "                                     Tokenized words  \n",
       "0  [We, had, my, Mother, s, Birthday, Party, here...  \n",
       "1  [Good, Korean, grill, near, Eaton, Centre, The...  \n",
       "2  [Was, recommended, to, try, this, place, by, f...  \n",
       "3  [Ambience, Would, not, expect, something, this...  \n",
       "4  [Absolutely, the, WORST, pool, company, that, ...  "
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df.to_csv('../Results/Tokenize_gensim.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('myenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "interpreter": {
   "hash": "03f7dc19a1cedc573e2ca4eb2fb8649d76cbed23038ca76b3f3820d1f93f421c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}